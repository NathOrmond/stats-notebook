{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Geometric Distribution\n",
    "\n",
    "The geometric distribution models the number of trials needed to get the first success in a sequence of independent Bernoulli trials.\n",
    "\n",
    "## Key Properties\n",
    "\n",
    "1. **Definition**: If X represents the number of trials until first success:\n",
    "   * P(X = k) = (1-p)^(k-1) * p for k = 1, 2, 3, ...\n",
    "   * where p is probability of success on each trial\n",
    "\n",
    "2. **Parameters**:\n",
    "   * p: probability of success on each trial (0 < p ≤ 1)\n",
    "\n",
    "3. **Key Statistics**:\n",
    "   * E(X) = 1/p\n",
    "   * Var(X) = (1-p)/p²\n",
    "   * Mode = 1\n",
    "   * Median = ⌈-1/log₂(1-p)⌉\n",
    "\n",
    "4. **Properties**:\n",
    "   * Memoryless property (past trials don't affect future probability)\n",
    "   * Only takes positive integer values\n",
    "   * Right-skewed distribution\n",
    "   * Decreasing probability mass function\n",
    "\n",
    "## Cumulative Distribution Function (CDF)\n",
    "F(x) = \n",
    "* 0 for x < 1\n",
    "* 1 - (1-p)^⌊x⌋ for x ≥ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interactive, FloatSlider\n",
    "import ipywidgets as widgets\n",
    "import scipy.stats as stats \n",
    "print(\"setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_geometric(p):\n",
    "    \"\"\"\n",
    "    Create visualization of geometric distribution with given parameter\n",
    "    \"\"\"\n",
    "    # Create points for plotting (up to 95th percentile)\n",
    "    max_k = int(np.ceil(np.log(0.05)/np.log(1-p)))\n",
    "    k = np.arange(1, max_k + 1)\n",
    "    \n",
    "    # Create geometric distribution\n",
    "    geom = stats.geom(p)\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "    \n",
    "    # Plot PMF\n",
    "    ax1.stem(k, geom.pmf(k))  # Removed use_line_collection parameter\n",
    "    ax1.set_title(f'Probability Mass Function (PMF)\\nGeometric(p={p:.3f})')\n",
    "    ax1.grid(True)\n",
    "    ax1.set_ylabel('Probability')\n",
    "    ax1.set_xlabel('Number of trials until first success (k)')\n",
    "    \n",
    "    # Plot CDF\n",
    "    ax2.step(k, geom.cdf(k), where='post')\n",
    "    ax2.set_title('Cumulative Distribution Function (CDF)')\n",
    "    ax2.grid(True)\n",
    "    ax2.set_ylabel('Cumulative Probability')\n",
    "    ax2.set_xlabel('Number of trials until first success (k)')\n",
    "    \n",
    "    # Add statistics\n",
    "    stats_text = f'Mean: {geom.mean():.2f}\\n'\n",
    "    stats_text += f'Variance: {geom.var():.2f}\\n'\n",
    "    stats_text += f'Std Dev: {geom.std():.2f}'\n",
    "    plt.figtext(0.95, 0.95, stats_text, fontsize=10, ha='right', va='top')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create interactive widget\n",
    "interactive_plot = interactive(\n",
    "    plot_geometric,\n",
    "    p=FloatSlider(min=0.01, max=1, step=0.01, value=0.5, description='p:')\n",
    ")\n",
    "\n",
    "# Display the widget\n",
    "display(interactive_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Disease Testing\n",
    "\n",
    "Consider testing animals for a disease where:\n",
    "* Probability of finding an infected animal (p) = 0.01\n",
    "* We test until we find the first infected animal\n",
    "\n",
    "Key calculations:\n",
    "1. Expected number of tests needed = 1/p = 100 tests\n",
    "2. Probability of finding infected animal within first 50 tests = 1 - (1-0.01)^50 ≈ 0.395\n",
    "\n",
    "Try adjusting the probability below to explore different scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Applications\n",
    "\n",
    "The geometric distribution is used in many real-world scenarios:\n",
    "\n",
    "1. **Quality Control**:\n",
    "   * Number of items inspected until finding first defect\n",
    "   * Tests until first failure\n",
    "\n",
    "2. **Risk Analysis**:\n",
    "   * Time until first accident\n",
    "   * Attempts until first security breach\n",
    "\n",
    "3. **Communication Systems**:\n",
    "   * Transmission attempts until successful delivery\n",
    "   * Network packet retries\n",
    "\n",
    "4. **Medical Testing**:\n",
    "   * Screening until finding first positive case\n",
    "   * Treatment attempts until success\n",
    "\n",
    "## Assumptions and Limitations\n",
    "\n",
    "1. **Key Assumptions**:\n",
    "   * Independent trials (memoryless)\n",
    "   * Constant probability of success\n",
    "   * Trials continue until success\n",
    "\n",
    "2. **Limitations**:\n",
    "   * May not account for learning or fatigue\n",
    "   * Assumes unlimited possible trials\n",
    "   * Success probability might change in practice\n",
    "\n",
    "## Relationship to Other Distributions\n",
    "\n",
    "1. Special case of negative binomial (r=1)\n",
    "2. Discrete analogue of exponential distribution\n",
    "3. Related to Bernoulli trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simulation of geometric distribution\n",
    "np.random.seed(42)\n",
    "p = 0.2  # probability of success\n",
    "n_simulations = 1000\n",
    "\n",
    "# Simulate geometric random variables\n",
    "data = stats.geom.rvs(p, size=n_simulations)\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(data, bins=range(1, max(data) + 2), density=True, alpha=0.7, \n",
    "         color='skyblue', label='Simulated data')\n",
    "\n",
    "# Add theoretical PMF\n",
    "k = np.arange(1, 20)\n",
    "pmf = stats.geom.pmf(k, p)\n",
    "# Note: removed use_line_collection parameter here\n",
    "plt.stem(k, pmf, label='Theoretical PMF', \n",
    "         linefmt='r-', markerfmt='ro')\n",
    "\n",
    "plt.title(f'Histogram of Simulated Geometric Data (p={p})')\n",
    "plt.xlabel('Number of trials until success')\n",
    "plt.ylabel('Probability')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Compare empirical and theoretical means\n",
    "print(f'Theoretical mean: {1/p:.2f}')\n",
    "print(f'Simulated mean: {np.mean(data):.2f}')\n",
    "print(f'Theoretical variance: {(1-p)/p**2:.2f}')\n",
    "print(f'Simulated variance: {np.var(data):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Details\n",
    "\n",
    "1. **Moment Generating Function**:\n",
    "   * M(t) = pe^t/(1-(1-p)e^t) for t < -ln(1-p)\n",
    "\n",
    "2. **Probability Generating Function**:\n",
    "   * G(z) = pz/(1-(1-p)z) for |z| < 1/(1-p)\n",
    "\n",
    "3. **Memoryless Property**:\n",
    "   * P(X > n + k | X > n) = P(X > k)\n",
    "   * This is a key characteristic shared with exponential distribution\n",
    "\n",
    "## Practice Problems\n",
    "\n",
    "1. If p = 0.2, what is:\n",
    "   * P(X = 3)?\n",
    "   * P(X ≤ 4)?\n",
    "   * E(X)?\n",
    "\n",
    "2. In testing for a rare condition (p = 0.01):\n",
    "   * What's the expected number of tests needed?\n",
    "   * What's the probability of finding a case within 10 tests?\n",
    "   * What's the standard deviation of the number of tests needed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Inverse Gambler's Fallacy and Geometric Distribution\n",
    "\n",
    "The Inverse Gambler's Fallacy is the incorrect belief that, upon observing a rare event, it's more likely to have occurred within a longer sequence of trials rather than a shorter one.\n",
    "\n",
    "### Key Concepts:\n",
    "1. **Geometric Distribution**:\n",
    "   * Models waiting time until first success\n",
    "   * P(X = k) = (1-p)^(k-1) * p\n",
    "   * Each trial is independent\n",
    "\n",
    "2. **Inverse Gambler's Fallacy**:\n",
    "   * Incorrectly reasons backward from observing success\n",
    "   * Confuses P(Many trials | Success) with P(Success | Many trials)\n",
    "   * Violates independence assumption\n",
    "\n",
    "3. **Why It's a Fallacy**:\n",
    "   * Success on current trial has probability p\n",
    "   * Previous trials don't affect this probability\n",
    "   * Can't infer trial number from single observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_inverse_gamblers_fallacy(p, n_sequences=10000):\n",
    "    \"\"\"\n",
    "    Demonstrate the difference between:\n",
    "    1. P(Success on trial k)\n",
    "    2. P(This is trial k | Success)\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate many geometric sequences\n",
    "    first_successes = stats.geom.rvs(p, size=n_sequences)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 15))\n",
    "    \n",
    "    # Plot 1: Geometric PMF\n",
    "    k = np.arange(1, 15)\n",
    "    pmf = stats.geom.pmf(k, p)\n",
    "    ax1.stem(k, pmf, use_line_collection=True)\n",
    "    ax1.set_title('Geometric Distribution PMF\\nP(First success on trial k)')\n",
    "    ax1.set_xlabel('Trial number (k)')\n",
    "    ax1.set_ylabel('Probability')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot 2: Probability of success given trial number\n",
    "    p_success = np.full_like(k, p)  # constant probability\n",
    "    ax2.stem(k, p_success, use_line_collection=True)\n",
    "    ax2.set_title('Probability of Success Given Trial Number\\nP(Success | This is trial k)')\n",
    "    ax2.set_xlabel('Trial number (k)')\n",
    "    ax2.set_ylabel('Probability')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # Plot 3: Conditional probability demonstration\n",
    "    # Given we saw a success, what's the probability it was on trial k?\n",
    "    counts = np.bincount(first_successes)[1:]  # remove 0 index\n",
    "    prob_k_given_success = counts / n_sequences\n",
    "    ax3.stem(np.arange(1, len(prob_k_given_success) + 1), \n",
    "             prob_k_given_success[:14], use_line_collection=True)\n",
    "    ax3.set_title('Empirical P(This is trial k | We observe a success)')\n",
    "    ax3.set_xlabel('Trial number (k)')\n",
    "    ax3.set_ylabel('Probability')\n",
    "    ax3.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print some insights\n",
    "    print(\"\\nKey Insights:\")\n",
    "    print(f\"1. P(Success on any single trial) = {p}\")\n",
    "    print(f\"2. Expected number of trials until success = {1/p:.1f}\")\n",
    "    print(f\"3. P(Success within first 5 trials) = {1 - (1-p)**5:.3f}\")\n",
    "    \n",
    "# Demonstrate with a rare event (p=0.1)\n",
    "demonstrate_inverse_gamblers_fallacy(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Plots Above\n",
    "\n",
    "1. **Top Plot (Geometric PMF)**:\n",
    "   * Shows probability of first success occurring on trial k\n",
    "   * Decreasing because we need k-1 failures before success\n",
    "   * This is a forward probability\n",
    "\n",
    "2. **Middle Plot (Success Given Trial)**:\n",
    "   * Shows probability of success on any given trial\n",
    "   * Constant because trials are independent\n",
    "   * This is what the fallacy misunderstands\n",
    "\n",
    "3. **Bottom Plot (Trial Given Success)**:\n",
    "   * Shows empirical probability that a success came from trial k\n",
    "   * Matches geometric distribution\n",
    "   * This is what we actually observe\n",
    "\n",
    "### Casino Example\n",
    "\n",
    "Imagine walking into a casino and seeing someone roll double sixes (p = 1/36):\n",
    "\n",
    "1. **Correct Reasoning**:\n",
    "   * This roll had 1/36 chance regardless of previous rolls\n",
    "   * We can't infer anything about number of previous rolls\n",
    "   * Each roll is independent\n",
    "\n",
    "2. **Fallacious Reasoning**:\n",
    "   * \"This was unlikely (1/36)\"\n",
    "   * \"More likely to see it after many rolls\"\n",
    "   * \"Therefore, many rolls probably happened\"\n",
    "\n",
    "3. **Why It's Wrong**:\n",
    "   * Geometric distribution tells us about waiting time until first success\n",
    "   * It doesn't tell us about past history given current success\n",
    "   * Current success probability is always p regardless of history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_probabilities(p=1/36, n_trials=10):\n",
    "    \"\"\"\n",
    "    Compare different probability calculations relevant to the fallacy\n",
    "    \"\"\"\n",
    "    # Probability of success on exactly trial k\n",
    "    k = np.arange(1, n_trials + 1)\n",
    "    p_first_on_k = stats.geom.pmf(k, p)\n",
    "    \n",
    "    # Probability of at least one success within k trials\n",
    "    p_within_k = 1 - (1-p)**k\n",
    "    \n",
    "    # Create comparison plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(k, p_first_on_k, 'b-', label='P(First success on trial k)')\n",
    "    plt.plot(k, p_within_k, 'r-', label='P(At least one success within k trials)')\n",
    "    plt.axhline(y=p, color='g', linestyle='--', \n",
    "                label='P(Success on any single trial)')\n",
    "    \n",
    "    plt.title('Probability Comparisons for Double Sixes (p=1/36)')\n",
    "    plt.xlabel('Number of trials (k)')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print key probabilities\n",
    "    print(\"\\nKey Probabilities for Double Sixes:\")\n",
    "    print(f\"P(Success on any single roll) = {p:.4f}\")\n",
    "    print(f\"P(First success on roll 10) = {p_first_on_k[9]:.4f}\")\n",
    "    print(f\"P(At least one success within 10 rolls) = {p_within_k[9]:.4f}\")\n",
    "    print(f\"Expected number of rolls until first success = {1/p:.1f}\")\n",
    "\n",
    "compare_probabilities()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare that plot to this plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_some_vs_this(p=1/36, max_rolls=50):\n",
    "    \"\"\"\n",
    "    Demonstrate how probability of seeing SOME success increases with sequence length,\n",
    "    while probability of THIS particular roll being a success stays constant\n",
    "    \"\"\"\n",
    "    # Create sequence lengths\n",
    "    rolls = np.arange(1, max_rolls + 1)\n",
    "    \n",
    "    # Calculate probabilities\n",
    "    p_some = 1 - (1-p)**rolls  # Probability of SOME success within n rolls\n",
    "    p_this = np.full_like(rolls, p)  # Probability of THIS roll being success\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "    \n",
    "    # Plot probabilities\n",
    "    ax1.plot(rolls, p_some, 'b-', label='P(SOME roll is double sixes)')\n",
    "    ax1.plot(rolls, p_this, 'r--', label='P(THIS roll is double sixes)')\n",
    "    ax1.set_title('Probability of Seeing Double Sixes')\n",
    "    ax1.set_xlabel('Number of Rolls')\n",
    "    ax1.set_ylabel('Probability')\n",
    "    ax1.grid(True)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Add annotations for specific points\n",
    "    rolls_of_interest = [10, 20, 30, 40]\n",
    "    for n in rolls_of_interest:\n",
    "        prob = 1 - (1-p)**n\n",
    "        ax1.annotate(f'n={n}: {prob:.3f}', \n",
    "                    xy=(n, prob), \n",
    "                    xytext=(5, 5),\n",
    "                    textcoords='offset points')\n",
    "    \n",
    "    # Plot relative likelihood\n",
    "    relative_likelihood = p_some/p\n",
    "    ax2.plot(rolls, relative_likelihood, 'g-', \n",
    "             label='How many times more likely to see SOME vs THIS')\n",
    "    ax2.set_title('Relative Likelihood: SOME vs THIS')\n",
    "    ax2.set_xlabel('Number of Rolls')\n",
    "    ax2.set_ylabel('Factor increase in likelihood')\n",
    "    ax2.grid(True)\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print key insights\n",
    "    print(\"\\nKey Probabilities for Double Sixes:\")\n",
    "    print(f\"P(THIS roll is double sixes) = {p:.4f}\")\n",
    "    print(\"\\nP(SOME roll is double sixes within n rolls):\")\n",
    "    for n in rolls_of_interest:\n",
    "        prob = 1 - (1-p)**n\n",
    "        print(f\"n = {n}: {prob:.4f} ({prob/p:.1f}x more likely than single roll)\")\n",
    "    \n",
    "    print(f\"\\nExpected number of rolls until first double sixes = {1/p:.1f}\")\n",
    "\n",
    "# Run demonstration\n",
    "demonstrate_some_vs_this()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the SOME vs THIS Probability Plot\n",
    "\n",
    "### Top Plot: Probability Comparison\n",
    "The top plot shows two crucial probabilities:\n",
    "\n",
    "1. **Blue Line (SOME)**: P(See double sixes at least once in n rolls)\n",
    "   * Starts at p=1/36 ≈ 0.028 for one roll\n",
    "   * Increases with number of rolls\n",
    "   * Follows formula: 1 - (35/36)^n\n",
    "   * Shows why longer sequences are more likely to contain double sixes\n",
    "   * Reaches about 0.75 by 50 rolls\n",
    "\n",
    "2. **Red Line (THIS)**: P(This specific roll is double sixes)\n",
    "   * Constant at p=1/36 ≈ 0.028\n",
    "   * Doesn't change with sequence length\n",
    "   * Shows why the Inverse Gambler's Fallacy is wrong\n",
    "   * Past rolls don't affect current probability\n",
    "\n",
    "### Bottom Plot: Relative Likelihood\n",
    "Shows how many times more likely you are to see double sixes at least once in n rolls compared to a single roll:\n",
    "\n",
    "* At 10 rolls: About 10 times more likely to see SOME doubles sixes\n",
    "* At 20 rolls: About 20 times more likely\n",
    "* At 50 rolls: About 27 times more likely\n",
    "\n",
    "### Key Insights\n",
    "1. While longer sequences are indeed more likely to contain SOME double sixes:\n",
    "   * This doesn't tell us anything about which specific roll will be successful\n",
    "   * Can't work backwards from seeing a success to infer sequence length\n",
    "\n",
    "2. The Inverse Gambler's Fallacy occurs when people:\n",
    "   * See the increasing blue line (SOME probability)\n",
    "   * Incorrectly apply it to the red line (THIS probability)\n",
    "   * Forget that each individual roll remains at 1/36\n",
    "\n",
    "3. This demonstrates why:\n",
    "   * Casinos can truthfully advertise \"Our tables see double sixes multiple times per hour!\"\n",
    "   * While each individual roll still has only 1/36 probability\n",
    "   * Both statements are compatible and not contradictory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Lessons\n",
    "\n",
    "1. **Independence Matters**:\n",
    "   * Each trial has same probability regardless of history\n",
    "   * Geometric distribution describes forward probability only\n",
    "   * Can't use it to reason backward about number of trials\n",
    "\n",
    "2. **Common Misunderstandings**:\n",
    "   * Confusing P(A|B) with P(B|A)\n",
    "   * Ignoring independence of trials\n",
    "   * Misapplying waiting time distributions\n",
    "\n",
    "3. **Statistical Inference**:\n",
    "   * Single observations don't tell us about sequence length\n",
    "   * Need additional information to make claims about history\n",
    "   * Important in scientific reasoning and data analysis"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
